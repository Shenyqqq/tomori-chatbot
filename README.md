# tomori-chatbot

An AI chatbot designed to simulate the character Takamatsu Tomori from *BanG Dream\! It's MyGO\!\!\!\!\!*.

The project integrates a LoRA-tuned large language model with a Retrieval-Augmented Generation (RAG) system for context-aware responses. The front end is a Gradio web UI which features a Live2D model whose expressions are dynamically controlled by the language model's output.

[](https://github.com/Shenyqqq/tomori-chatbot)
[](https://www.google.com/search?q=https://github.com/Shenyqqq/tomori-chatbot/blob/main/LICENSE)
[](https://www.python.org/downloads/)

-----

## Features

  * **Character-Specific Dialogue**: Generates text in the persona of a specific character using a LoRA-finetuned Qwen2.5-7B-int4 model.
  * **Retrieval-Augmented Generation (RAG)**: Enhances responses with relevant context by retrieving information from a vector knowledge base before generation.
  * **Dynamic Live2D Expressions**: Parses the generated text to determine an emotional sentiment, which then triggers the corresponding animation in a Live2D model rendered on the web UI.
  * **Web Interface**: Built with Gradio for straightforward user interaction and demonstration.

## Demo

*(A screenshot or GIF of the running application is recommended here.)*

\![Demo Screenshot/GIF]([Link to your screenshot or GIF])

## System Architecture

The application operates on the following data flow:

*(A simple block diagram is recommended to visualize this flow.)*

1.  **Input**: The user submits a text string through the Gradio web interface.
2.  **Retrieval**: The input string is converted to an embedding and used to perform a similarity search against a pre-built vector database (FAISS/ChromaDB). The top-k results are retrieved as context.
3.  **Prompt Construction**: A final prompt is assembled using a template that includes the system persona, the retrieved context, and the user's original input.
4.  **Generation**: The constructed prompt is passed to the Qwen2.5-7B model with the attached LoRA adapter. The model generates the response text.
5.  **Sentiment Analysis**: The generated text is processed by a lightweight function (e.g., keyword-based classifier) to map it to a predefined emotion category (e.g., `neutral`, `happy`, `sad`).
6.  **Output**: The generated text and the emotion category are sent back to the Gradio frontend. The text is displayed in the chat log, and the emotion category is used by a JavaScript listener to trigger the appropriate Live2D expression.

## Technology Stack

  * **LLM**: `Qwen2.5-7B-int4`
  * **Fine-tuning**: `LoRA (Low-Rank Adaptation)`
  * **Backend**: `Python`, `Gradio`
  * **Frontend**: `HTML`, `JavaScript`
  * **Animation**: `Live2D Cubism SDK for Web`
  * **RAG**: Vector database library such as `FAISS` or `ChromaDB`.

## Data and Fine-tuning

The model's performance relies on a multi-source dataset strategy for both fine-tuning and retrieval.

### Fine-tuning Data

The LoRA adapter was trained on a composite dataset:

1.  **Game Scripts**: Base dialogue extracted directly from game data files. This provided the foundational character persona but was insufficient in volume and conversational quality.
2.  **LLM-Augmented QA**: A large set of question-answer pairs generated by an auxiliary LLM. This was created to expand the dataset, improve conversational flow, and mitigate the limitations of the original script data.
3.  **General Dialogue Data**: A small, curated set of generic conversational data was included to improve the model's generalization and prevent overfitting.

### RAG Knowledge Base

For efficiency, the **LLM-Augmented QA dataset** was repurposed as the knowledge base for the RAG system. This approach ensures that the retrieved context is stylistically consistent with the fine-tuned model's expected output.

## Setup and Installation

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/Shenyqqq/tomori-chatbot.git
    cd tomori-chatbot
    ```

2.  **Create a Python virtual environment and install dependencies:**

    ```bash
    # It is recommended to use a virtual environment
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    pip install -r requirements.txt
    ```

3.  **Download required models:**

      * Place the base LLM (e.g., `Qwen2.5-7B-Chat-GGUF`) in the `models/` directory.
      * Place your trained LoRA adapter weights in the `lora_weights/` directory.
      * Place the Live2D model assets (`.moc3`, `.model3.json`, textures) in the `static/live2d/` directory.
      * Ensure your RAG vector database/index file is accessible at the path configured in the application.

4.  **Run the application:**

    ```bash
    python app.py
    ```

    The web interface will be available at `http://127.0.0.1:7860`.

## Future Development

  * **TTS Integration**: Implement a Text-to-Speech model for voice output.
  * **Stateful Memory**: Add a mechanism for long-term memory to maintain conversation context across sessions.
  * **Multi-Modal Input**: Extend functionality to accept image inputs.
